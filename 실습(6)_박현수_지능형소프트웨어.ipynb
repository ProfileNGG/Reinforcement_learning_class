{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhhwipZHjbHnfhDc4Av99Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfileNGG/Reinforcement_learning_class/blob/main/%EC%8B%A4%EC%8A%B5(6)_%EB%B0%95%ED%98%84%EC%88%98_%EC%A7%80%EB%8A%A5%ED%98%95%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X1R57jJD7kFF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# 1. 초기화 for Markov Reward Process = (S, P, R, gamma)\n",
        "# (1) S: state 정의: 8개의 상태\n",
        "states = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "# (2) P: state-transition matrix 정의: 8x8 matrix\n",
        "P = np.array([\n",
        "  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "  [0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0],\n",
        "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2],\n",
        "  [0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.4, 0.0],\n",
        "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
        "  [0.0, 0.1, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0],\n",
        "  [0.0, 0.2, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0],\n",
        "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "])\n",
        "# (3) R: reward 정의\n",
        "R = np.array([0, -2, -2, -2, 10, -1, 1, 0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Value iteration 구현\n",
        "# (1) state value function 초기\n",
        "V = np.zeros(len(states))\n",
        "threshold = 0.0001 # threshold 설정"
      ],
      "metadata": {
        "id": "ApE2z9ha84yI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration (gamma):\n",
        "  # (2) iteration\n",
        "  while True:\n",
        "    # (2-1) V_new <-- V\n",
        "    V_new = np.copy(V)\n",
        "\n",
        "    # (2-2) V_new 계산\n",
        "    for s in states:\n",
        "     V_new[s] = R[s] + gamma * np.sum([P[s, s_prime] * V[s_prime] for s_prime in states])\n",
        "\n",
        "    # (2-3) |V - V_new} < threshold --> break\n",
        "    if np.max(np.abs(V_new - V)) < threshold:\n",
        "      V = V_new\n",
        "      break\n",
        "\n",
        "    # (2-4) V <-- V_new\n",
        "    V = V_new\n",
        "  print(\"Value function:\", V)\n"
      ],
      "metadata": {
        "id": "lAPnZ6_J87w2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. MDP 정의\n",
        "# (1) S\n",
        "shape = (4, 4)\n",
        "terminals = [(0, 0), (3, 3)]\n",
        "\n",
        "# (2) A\n",
        "numa = 4\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# (3) P\n",
        "def P(state, action):\n",
        "  if action == 'up':\n",
        "    next_state = (max(0, state[0]-1), state[1])\n",
        "  elif action == 'down':\n",
        "    next_state = (min(shape[0]-1, state[0]+1), state[1])\n",
        "  elif action == 'left':\n",
        "    next_state = (state[0], max(0, state[1]-1))\n",
        "  elif action == 'right':\n",
        "    next_state = (state[0], min(shape[1]-1, state[1]+1))\n",
        "  return next_state\n",
        "\n",
        "# (4) R\n",
        "reward = -1 * np.ones(shape)\n",
        "for terminal in terminals:\n",
        "  reward[terminal] = 0\n",
        "\n",
        "# (5) gamma\n",
        "gamma = 1.0"
      ],
      "metadata": {
        "id": "tCnObOF89NrX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. value iteration\n",
        "# (1) Initialize the value function\n",
        "V = np.zeros(shape)\n",
        "\n",
        "# (2) Value iteration\n",
        "while True:\n",
        "  delta = 0\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      if(i,j) in terminals:\n",
        "       continue\n",
        "      v = V[i,j]\n",
        "      V[i,j] = sum((reward[i, j] + gamma * V[P((i,j), a)]) for a in actions)/ numa\n",
        "      delta = max(delta, abs(v-V[i,j]))\n",
        "  if delta < 1e-4:\n",
        "    break"
      ],
      "metadata": {
        "id": "O0vyeE-t9ZSP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. value iteration\n",
        "# (3) Extract the optimal policy\n",
        "optimal_policy = {}\n",
        "for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      if(i,j) in terminals:\n",
        "         optimal_policy[i, j] = 'terminal'\n",
        "      else:\n",
        "        optimal_policy[i, j] = actions[np.argmax([(reward[i, j] + gamma * V[P((i, j), a)]) for a in actions])]"
      ],
      "metadata": {
        "id": "VK7XUwzX9ZXk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 결과 출력\n",
        "print(\"Optimal policy is:\")\n",
        "for i in range(shape[0]):\n",
        "  for j in range(shape[1]):\n",
        "    print(f\"({i},{j}): {optimal_policy[i,j]}\")\n",
        "\n",
        "print(\"\\nOptimal value function is:\")\n",
        "for i in range(shape[0]):\n",
        "  for j in range(shape[1]):\n",
        "   print(f\"V({i},{j}): {V[i,j]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaXhaBItzajY",
        "outputId": "cfa75260-acf6-4bb0-dc6d-e17c029ab00f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy is:\n",
            "(0,0): terminal\n",
            "(0,1): left\n",
            "(0,2): left\n",
            "(0,3): left\n",
            "(1,0): up\n",
            "(1,1): up\n",
            "(1,2): left\n",
            "(1,3): down\n",
            "(2,0): up\n",
            "(2,1): up\n",
            "(2,2): right\n",
            "(2,3): down\n",
            "(3,0): up\n",
            "(3,1): right\n",
            "(3,2): right\n",
            "(3,3): terminal\n",
            "\n",
            "Optimal value function is:\n",
            "V(0,0): 0.0\n",
            "V(0,1): -13.999312424461952\n",
            "V(0,2): -19.999011518162753\n",
            "V(0,3): -21.998911992496346\n",
            "V(1,0): -13.999312424461952\n",
            "V(1,1): -17.999156254598965\n",
            "V(1,2): -19.99908388638086\n",
            "V(1,3): -19.99909436158647\n",
            "V(2,0): -19.999011518162757\n",
            "V(2,1): -19.99908388638086\n",
            "V(2,2): -17.99922696784339\n",
            "V(2,3): -13.999422844683943\n",
            "V(3,0): -21.99891199249635\n",
            "V(3,1): -19.999094361586472\n",
            "V(3,2): -13.999422844683945\n",
            "V(3,3): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transition(state, action):\n",
        "\n",
        "    next_state_dict = {\n",
        "        ((0, 0), 'up'): (0, 1),\n",
        "\n",
        "    }\n",
        "    return next_state_dict.get((state, action), state)\n",
        "\n",
        "policy = {state: np.random. choice (actions) for state in  [(i, j) for i in range (shape[0]) for j in range (shape [1])] if state not in terminals}\n",
        "\n",
        "# Policy iteration\n",
        "while True:\n",
        "  # Policy evaluation\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for i in range (shape [0]):\n",
        "      for j in range (shape[1]):\n",
        "        if (i, j) in terminals:\n",
        "          continue\n",
        "        v = V[i, j]\n",
        "        action = policy[(i, j)]\n",
        "        V[i, j] = reward [i, j] + gamma * V[P ((i, j), action)]\n",
        "        delta = max (delta, abs (v - V[i, j]))\n",
        "\n",
        "\n",
        "    if delta < 2.01:\n",
        "      break"
      ],
      "metadata": {
        "id": "MD5aQX3Gz_Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy improvement\n",
        "  policy_stable = True\n",
        "  for i in range (shape [0]):\n",
        "    for j in range (shape[1]):\n",
        "      if (i, j) in terminals:\n",
        "        continue\n",
        "\n",
        "      old_action = policy[(i, j)]\n",
        "      policy[(i, j)] = actions [np.argmax ([reward [i, j] + gamma * V[transition ((i, j), action)] for action in actions])]\n",
        "\n",
        "      if old_action != policy [(i, j)]:\n",
        "        policy_stable  = False\n",
        "  if policy_stable:\n",
        "    break"
      ],
      "metadata": {
        "id": "lD3CZgEM4_7b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}